{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision Transformers Flax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMggD0dcDtFuMuw5OmF2htb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayureshagashe2105/GSoC-22-TensorFlow-Resources-and-Notebooks/blob/main/JAX/Vision_Transformers_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwdkoZW6XkdA",
        "outputId": "4c31566c-6b70-4578-f2a3-06c5df529377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 150 kB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pip jax jaxlib\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import lax, random, numpy as jnp, jit\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.core.lift import vmap\n",
        "# from flax.linen.transforms import jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "q-nusa4rdzsE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(tf.keras.utils.Sequence):\n",
        "  def __init__(self, batch_size, X, y, is_training):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.batch_size = batch_size\n",
        "    self.is_training = is_training\n",
        "    self.indices = range(X.shape[0])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.shape[0] // self.batch_size\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    batch_indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "    batch_images = self.X[batch_indices]\n",
        "    batch_labels = self.y[batch_indices]\n",
        "\n",
        "    return jnp.array(batch_images), jnp.array(batch_labels)\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = ((tf.cast(X_train, tf.float32).numpy() / 255.0, tf.cast(y_train, tf.int32).numpy()), \n",
        "                                        (tf.cast(X_test, tf.float32).numpy() / 255.0, tf.cast(y_test, tf.int32).numpy()))\n",
        "\n",
        "\n",
        "gen = DataLoader(64, X_train[:1000], y_train[:1000], True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U71njssJeX-A",
        "outputId": "93b26b1d-8fe3-480d-a7b9-e46316b79a9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \n",
        "  hidden_layer_nodes: Sequence[int]\n",
        "  activation: str\n",
        "\n",
        "  def setup(self):\n",
        "    whitelist_activations = ['celu', 'elu', 'gelu', 'glu', 'log_sigmoid', 'log_softmax', 'relu', 'sigmoid', 'soft_sign', 'softmax', 'softplus', 'swish', 'PRelu', None]\n",
        "    if self.activation not in whitelist_activations:\n",
        "      raise ValueError(f'{self.activation} should be one of {whitelist_activations}')\n",
        "\n",
        "\n",
        "    self.layers = [nn.Dense(n) for n in self.hidden_layer_nodes]\n",
        "  \n",
        "  @nn.compact\n",
        "  def __call__(self, input):\n",
        "    for layer in self.layers:\n",
        "      x = layer(input)\n",
        "      if self.activation is not None:\n",
        "        x = self.apply_activation(x)\n",
        "      return x\n",
        "  \n",
        "  def apply_activation(self, input):\n",
        "    if self.activation == 'celu': return nn.celu(input)\n",
        "    elif self.activation == 'elu': return nn.elu(input)\n",
        "    elif self.activation == 'gelu': return nn.gelu(input)\n",
        "    elif self.activation == 'glu': return nn.glu(input)\n",
        "    elif self.activation == 'log_sigmoid': return nn.log_sigmoid(input)\n",
        "    elif self.activation == 'log_softmax': return nn.log_softmax(input)\n",
        "    elif self.activation == 'relu': return nn.relu(input)\n",
        "    elif self.activation == 'sigmoid': return nn.sigmoid(input)\n",
        "    elif self.activation == 'soft_sign': return nn.soft_sign(input)\n",
        "    elif self.activation == 'softmax': return nn.softmax(input)\n",
        "    elif self.activation == 'softplus': return nn.softplus(input)\n",
        "    elif self.activation == 'swish': return nn.swish(input)\n",
        "    elif self.activation == 'PRelu': return nn.PRelu(input)"
      ],
      "metadata": {
        "id": "_KGSRTARe8Rm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(nn.Module):\n",
        "  num_patches: int\n",
        "  projection_dims: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.projection = nn.Dense(self.projection_dims)\n",
        "    self.positional_encodings = nn.Embed(self.num_patches, self.projection_dims)\n",
        "  \n",
        "  @nn.compact\n",
        "  def __call__(self, patch):\n",
        "    positions = jnp.arange(0, self.num_patches)\n",
        "    encode = self.projection(patch) + self.positional_encodings(positions)\n",
        "    return encode"
      ],
      "metadata": {
        "id": "k1fzvHoLfFEm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  patch_size: Sequence[int]\n",
        "  stride: int\n",
        "  image_size: Sequence[int]\n",
        "  activation: str\n",
        "  projection_dims: int\n",
        "  num_heads: int\n",
        "  transformer_layers: int\n",
        "  mlp_head_units: Sequence[int]\n",
        "  batch_size: int\n",
        "  num_classes: int\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "    self.transformer_units = [self.projection_dims * 2, self.projection_dims]\n",
        "    self.num_patches = ((self.image_size[0] - self.patch_size[0]) // self.stride + 1) * ((self.image_size[1] - self.patch_size[1]) // self.stride + 1)\n",
        "    \n",
        "    self.norm = nn.LayerNorm(epsilon=1e-6)\n",
        "    self.multi_head_attention = nn.MultiHeadDotProductAttention(num_heads=self.num_heads, qkv_features=self.projection_dims)\n",
        "    self.dropout10 = nn.Dropout(0.1)\n",
        "    self.dropout50 = nn.Dropout(0.5)\n",
        "    self.logits = nn.Dense(self.num_classes)\n",
        "\n",
        "    encode_init = PatchEncoder(self.num_patches, self.projection_dims)\n",
        "    mlp_init = MLP(self.transformer_units[::-1], self.activation)\n",
        "    mlp2_init = MLP(self.mlp_head_units[::-1], self.activation)\n",
        "\n",
        "    self.encode = encode_init\n",
        "    self.mlp_transformer = mlp_init\n",
        "    self.mlp_head = mlp2_init\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    # image_patches = self.extract_patches(inputs)\n",
        "\n",
        "    patches = jax.lax.conv_general_dilated_patches(inputs[:, None, None, :], (1, self.patch_size[0], self.patch_size[1], 1), \n",
        "                                                   (1, self.stride, self.stride, 1), \n",
        "                                                   padding=\"VALID\").reshape(self.batch_size, -1, self.patch_size[0] * self.patch_size[1] * inputs.shape[-1])\n",
        "    patch_dims = patches.shape[-1]\n",
        "    image_patches = patches.reshape(self.batch_size, -1, patch_dims)\n",
        "\n",
        "\n",
        "    encoded_image_patches = self.encode(image_patches)\n",
        "\n",
        "    for _ in range(self.transformer_layers):\n",
        "      x1 = self.norm(encoded_image_patches)\n",
        "      attention_output = self.multi_head_attention(x1, x1)\n",
        "      x2 = attention_output + encoded_image_patches #VisionTransformer.layer_add(attention_output, encoded_image_patches)\n",
        "      x3 = self.norm(x2)\n",
        "\n",
        "      x3 = self.mlp_transformer(x3)\n",
        "      # x3 = self.dropout10(x3, deterministic=not True)\n",
        "      # print(x3.shape)\n",
        "      encoded_image_patches = x3 + x2 #VisionTransformer.layer_add(x3, x2)\n",
        "    \n",
        "    repr = self.norm(encoded_image_patches)\n",
        "\n",
        "    repr = repr.reshape(self.batch_size, -1)\n",
        "    # repr = self.dropout50(repr, deterministic=not True)\n",
        "    repr = self.mlp_head(repr)\n",
        "    # print(repr.shape)\n",
        "    # repr = self.dropout(repr, deterministic=not True)\n",
        "    logit_nodes = self.logits(repr)\n",
        "    logit_nodes = nn.softmax(logit_nodes)\n",
        "\n",
        "    return logit_nodes\n",
        "  \n",
        "\n",
        "  @jit\n",
        "  def extract_patches(self, inputs):\n",
        "    patches = jax.lax.conv_general_dilated_patches(inputs[:, None, None, :], (1, self.patch_size[0], self.patch_size[1], 1), \n",
        "                                                   (1, self.stride, self.stride, 1), \n",
        "                                                   padding=\"VALID\").reshape(self.batch_size, -1, self.patch_size[0] * self.patch_size[1] * inputs.shape[-1])\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = patches.reshape(self.batch_size, -1, patch_dims)\n",
        "    return patches\n",
        "  \n",
        "  @staticmethod\n",
        "  @jit\n",
        "  def layer_add(x, y):\n",
        "    return jnp.add(x, y)"
      ],
      "metadata": {
        "id": "GnL6EwwRfIOw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATCH_SIZE = (10, 10)\n",
        "STRIDE = 10\n",
        "IMAGE_SIZE = (32, 32, 3)\n",
        "PROJECTION_DIMS = 64\n",
        "NUM_HEADS = 4\n",
        "TRANSFORMER_LAYERS = 8\n",
        "MLP_HEAD_UNITS = (2048, 1024)\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 10\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "SEED = 0\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "NCJtk-XAgXym"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(patch_size=PATCH_SIZE, stride=STRIDE, image_size=IMAGE_SIZE, activation=\"gelu\", projection_dims=PROJECTION_DIMS, num_heads=NUM_HEADS, \n",
        "                          transformer_layers=TRANSFORMER_LAYERS, mlp_head_units=MLP_HEAD_UNITS, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "ibj3rndWgP1m"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingLoop:\n",
        "  \n",
        "  def __init__(self, model, train_gen, seed, epochs, learning_rate, momentum, test_gen=None):\n",
        "    self.model = model\n",
        "    self.train_gen = train_gen\n",
        "    self.key = seed\n",
        "    self.rng = jax.random.PRNGKey(self.key)\n",
        "    self.epochs = epochs\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "\n",
        "    self.full_batch_size = (self.model.batch_size, self.model.image_size[0],\n",
        "                            self.model.image_size[1], self.model.image_size[2])\n",
        "    \n",
        "    self.init_train_state()\n",
        "\n",
        "\n",
        "\n",
        "  def init_train_state(self):\n",
        "    self.variables = self.model.init(self.rng, jnp.ones(self.full_batch_size))['params']\n",
        "    self.optimizer = optax.adam(self.lr, self.momentum)\n",
        "    self.train_state = train_state.TrainState.create(apply_fn = model.apply, tx=self.optimizer, params=self.variables)\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  @jax.jit\n",
        "  def apply_model(state, images, labels, num_classes):  \n",
        "    \n",
        "    def loss_fn(params):\n",
        "      logits = state.apply_fn({'params': params}, images)\n",
        "      one_hot = jax.nn.one_hot(labels, 10)\n",
        "      loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "      return loss, logits\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "    return grads, loss, accuracy\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  @jax.jit\n",
        "  def update_model(state, grads):\n",
        "    return state.apply_gradients(grads=grads)\n",
        "\n",
        " \n",
        "  @staticmethod\n",
        "  def train_epoch(state, gen, batch_size, rng, num_classes):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "\n",
        "    for (batch_images, batch_labels) in gen:\n",
        "      batch_images = batch_images\n",
        "      batch_labels = batch_labels\n",
        "      grads, loss, accuracy = TrainingLoop.apply_model(state, batch_images, batch_labels, 10)\n",
        "      state = TrainingLoop.update_model(state, grads)\n",
        "      epoch_loss.append(loss)\n",
        "      epoch_accuracy.append(accuracy)\n",
        "    \n",
        "    train_loss = np.mean(epoch_loss)\n",
        "    train_accuracy = np.mean(epoch_accuracy)\n",
        "    return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def train(obj):\n",
        "\n",
        "\n",
        "    for epoch in range(1, obj.epochs + 1):\n",
        "      state, train_loss, train_accuracy = TrainingLoop.train_epoch(obj.train_state, obj.train_gen,\n",
        "                                                      obj.model.batch_size,\n",
        "                                                      obj.rng,\n",
        "                                                      obj.model.num_classes)\n",
        "      \n",
        "      print(\n",
        "          'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f'\n",
        "          % (epoch, train_loss, train_accuracy * 100)\n",
        "          )\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "RhPReYpUgTMQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainingLoop(model, gen, SEED, EPOCHS, LEARNING_RATE, MOMENTUM, test_gen=None)\n",
        "final_state = TrainingLoop.train(trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922pUO0kCtNd",
        "outputId": "d616fc0f-8ea5-46a0-e18d-3704e854585d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  1, train_loss: 2.3420, train_accuracy: 9.99\n",
            "epoch:  2, train_loss: 2.3420, train_accuracy: 9.99\n",
            "epoch:  3, train_loss: 2.3420, train_accuracy: 9.99\n",
            "epoch:  4, train_loss: 2.3420, train_accuracy: 9.99\n",
            "epoch:  5, train_loss: 2.3420, train_accuracy: 9.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n5N0dtbUDh39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}