{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision Transformers Flax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyManYgmpHJUcwmN8sqrjLbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayureshagashe2105/GSoC-22-TensorFlow-Resources-and-Notebooks/blob/main/JAX/Vision_Transformers_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwdkoZW6XkdA",
        "outputId": "2f758e8b-4a5a-451f-c817-bb91ef86a36f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 112 kB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pip jax jaxlib\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import lax, random, numpy as jnp, jit\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.core.lift import vmap\n",
        "\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Sequence, List, Union, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "q-nusa4rdzsE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATCH_SIZE = (7, 7)\n",
        "STRIDE = 7\n",
        "IMAGE_SIZE = (32, 32, 3)\n",
        "PROJECTION_DIMS = 8\n",
        "SELFA_HEADS = 2\n",
        "TRANSFORMER_LAYERS = 8\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 10\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "SEED = 0\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "NCJtk-XAgXym"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(tf.keras.utils.Sequence):\n",
        "  \"\"\"Generates batches of images and labels to pass to the model\n",
        "  \n",
        "  Args:\n",
        "    batch_size: int. Size of a batch to yield.\n",
        "    X: np.ndarray. Images from the dataset in the form of numpy array.\n",
        "    y: np.ndarray. Labels for `X`.\n",
        "  \"\"\"\n",
        "  def __init__(self, batch_size: int, X: np.ndarray, y: np.ndarray) -> Tuple[jnp.ndarray]:\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.batch_size = batch_size\n",
        "    self.indices = range(X.shape[0])\n",
        "  \n",
        "  def __len__(self):\n",
        "    \"\"\"Number of batch in the Sequence.\n",
        "\n",
        "    Returns:\n",
        "        The number of batches in the Sequence.\n",
        "    \"\"\"\n",
        "    return self.X.shape[0] // self.batch_size\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"Returns pre-processed batch at position `index`.\n",
        "\n",
        "    Args:\n",
        "        index: position of the batch in the Sequence.\n",
        "\n",
        "    Returns:\n",
        "        A batch.\n",
        "    \"\"\"\n",
        "    batch_indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "    batch_images = self.X[batch_indices]\n",
        "    batch_labels = self.y[batch_indices]\n",
        "\n",
        "    if(len(batch_images.shape) == 3): # convert \n",
        "      batch_images = jnp.expand_dims(batch_images, -1)\n",
        "\n",
        "    return jnp.array(batch_images), jnp.array(batch_labels)\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = ((tf.cast(X_train, tf.float32).numpy() / 255.0, tf.cast(y_train, tf.int32).numpy()), \n",
        "                                        (tf.cast(X_test, tf.float32).numpy() / 255.0, tf.cast(y_test, tf.int32).numpy()))\n",
        "\n",
        "\n",
        "gen = DataLoader(BATCH_SIZE, X_train[:500], y_train[:500])"
      ],
      "metadata": {
        "id": "U71njssJeX-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ced0b3-ed0c-4f4e-dcb6-e6bd2743087a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"Multi-layer perceptron dataclass.\n",
        "  \n",
        "  Args:\n",
        "    hidden_layer_nodes: Sequence[int]. Number of nodes in hidden layers.\n",
        "    activations: Sequence[str]. Activation functions to apply at each layer.\n",
        "  \n",
        "  Raises:\n",
        "    AssertionError: If length of `self.activations` is not same as length of `self.hidden_layer_nodes`.\n",
        "    ValueError: If any value from `self.activations` is not from allowed activation functions.\n",
        "  \"\"\"\n",
        "  hidden_layer_nodes: Sequence[int]\n",
        "  activations: Sequence[str]\n",
        "\n",
        "  def setup(self):\n",
        "    \n",
        "    assert len(self.hidden_layer_nodes) == len(self.activations), \"Activation function for each layer must be provided.\"\n",
        "\n",
        "    self.__whitelist_activations = ['celu', 'elu', 'gelu', 'glu', 'log_sigmoid',\n",
        "                                    'log_softmax', 'relu', 'sigmoid', \n",
        "                                    'soft_sign', 'softmax', 'softplus', \n",
        "                                    'swish', 'PRelu', 'Linear']\n",
        "\n",
        "    self.layers = [(nn.Dense(self.hidden_layer_nodes[n]), self.activations[n]) \n",
        "                  for n in range(len(self.hidden_layer_nodes)) \n",
        "                  if self.activations[n] in self.__whitelist_activations \n",
        "                  ]\n",
        "    \n",
        "    if len(self.layers) is not len(self.activations):\n",
        "      raise ValueError(f'Activation function should be one of the {self.__whitelist_activations}') \n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, input):\n",
        "    for layer, activation in self.layers:\n",
        "      x = layer(input)\n",
        "      x = self.apply_activation(x, activation)\n",
        "      return x\n",
        "  \n",
        "  def apply_activation(self, input, activation):\n",
        "    if activation == 'celu': return nn.celu(input)\n",
        "    elif activation == 'elu': return nn.elu(input)\n",
        "    elif activation == 'gelu': return nn.gelu(input)\n",
        "    elif activation == 'glu': return nn.glu(input)\n",
        "    elif activation == 'log_sigmoid': return nn.log_sigmoid(input)\n",
        "    elif activation == 'log_softmax': return nn.log_softmax(input)\n",
        "    elif activation == 'relu': return nn.relu(input)\n",
        "    elif activation == 'sigmoid': return nn.sigmoid(input)\n",
        "    elif activation == 'soft_sign': return nn.soft_sign(input)\n",
        "    elif activation == 'softmax': return nn.softmax(input)\n",
        "    elif activation == 'softplus': return nn.softplus(input)\n",
        "    elif activation == 'swish': return nn.swish(input)\n",
        "    elif activation == 'PRelu': return nn.PRelu(input)\n",
        "    elif activation == \"Linear\": return input\n"
      ],
      "metadata": {
        "id": "_KGSRTARe8Rm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchExtractor(nn.Module):\n",
        "  \"\"\"Custom module to extract patches from the images.\n",
        "  \n",
        "  Args:\n",
        "    patch_size: Sequence[int]. Image will be divided into patches of the desired size.\n",
        "    stride: int. Stride length to slide the window for patch extraction.\n",
        "  \n",
        "  Raises:\n",
        "    AssertionError: If `patch_size` is not a sequence with length = 2.\n",
        "  \"\"\"\n",
        "  patch_size: Sequence[int]\n",
        "  stride: int\n",
        "\n",
        "  def setup(self):\n",
        "    assert len(self.patch_size) == 2, \"length of `patch_size` should be equal to 2.\"\n",
        "\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, images):\n",
        "    patches = jax.lax.conv_general_dilated_patches(images[:, None, None, :], \n",
        "                                                   (1, self.patch_size[0], self.patch_size[1], 1), \n",
        "                                                   (1, self.stride, self.stride, 1), \n",
        "                                                   padding=\"VALID\")\n",
        "    n_patches = (images.shape[1] // self.patch_size[0]) * (images.shape[2] // self.patch_size[1])\n",
        "    patch_dims = self.patch_size[0] * self.patch_size[1] * images.shape[3]\n",
        "    image_patches = patches.reshape(images.shape[0], n_patches, patch_dims)\n",
        "\n",
        "    return image_patches"
      ],
      "metadata": {
        "id": "EFKzPHOSWcSz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "  \"\"\"Custom module to return learnable positional encodings (sin-cosin waves).\n",
        "  \n",
        "  Args:\n",
        "    seq_len: int. Lenght of the sequence of patches.\n",
        "    projection_dims: int. Number of dimensions for internal representation of the model.\n",
        "  \"\"\"\n",
        "  seq_len: int\n",
        "  projection_dims: int\n",
        "\n",
        "  def __call__(self):\n",
        "    res = jnp.ones((self.seq_len, self.projection_dims))\n",
        "    for i in range(self.seq_len):\n",
        "      for j in range(self.projection_dims):\n",
        "        res = res.at[i, j].set(jnp.sin(i / (10000 ** (j / self.projection_dims))) if j % 2 == 0 else jnp.cos(i / (10000 ** ((j - 1) / self.projection_dims))))\n",
        "    \n",
        "    return jnp.expand_dims(res, 0)"
      ],
      "metadata": {
        "id": "hWGtvTSHjFKV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  \"\"\"Vision Transformer module\n",
        "  \n",
        "  Args:\n",
        "    patch_size: Sequence[int]. Image will be divided into patches of the desired size.\n",
        "    stride: int. Stride length to slide the window for patch extraction.\n",
        "    image_size: Sequence[int]. Format: (H, W, C). Size of 1 image from the batch.\n",
        "    projection_dims: int. Number of dimensions for internal representation of the model.\n",
        "    atten_heads: int. Number of self attention heads to be used.\n",
        "    transformer_layers: int. Number of transformer encoders to be used.\n",
        "    batch_size: int. Size of a batch to yield.\n",
        "    num_classes. int. Number of target classes.\n",
        "  \n",
        "  Raises:\n",
        "    AssertionError: If input images are not in the format (N, H, W, C).\n",
        "  \"\"\"\n",
        "  patch_size: Sequence[int]\n",
        "  stride: int\n",
        "  image_size: Sequence[int]\n",
        "  projection_dims: int\n",
        "  atten_heads: int\n",
        "  transformer_layers: int\n",
        "  batch_size: int\n",
        "  num_classes: int\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "\n",
        "    self.patchify = PatchExtractor(self.patch_size, self.stride)\n",
        "    self.patch_dims = self.patch_size[0] * self.patch_size[1] * self.image_size[-1]\n",
        "    self.tokens = MLP([self.projection_dims, self.patch_dims], activations=['gelu', 'gelu'])\n",
        "    self.class_token = self.param(\"class_token\", lambda rng, shape: random.normal(rng, shape), (1, self.projection_dims))\n",
        "\n",
        "    self.num_patches = ((self.image_size[0] - self.patch_size[0]) // self.stride + 1) * ((self.image_size[1] - self.patch_size[1]) // self.stride + 1)\n",
        "    self.pos_encodings = PositionalEncodings(self.num_patches + 1, self.projection_dims)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(epsilon=1e-6)\n",
        "    self.self_attention = nn.SelfAttention(self.atten_heads, qkv_features=self.projection_dims)\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(epsilon=1e-6)\n",
        "    self.enc_mlp = MLP([self.projection_dims, self.projection_dims], activations=['gelu', 'relu'])\n",
        "\n",
        "    self.logits_mlp = MLP([self.num_classes, self.projection_dims], activations=['gelu', 'softmax'])\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    assert len(inputs.shape) == 4, f\"ViT encoder expected 4D vector as input in the format (N, H, W, C) but got {len(inputs.shape)}D vector instead.\"\n",
        "\n",
        "    image_patches = self.patchify(inputs)\n",
        "    tokens = self.tokens(image_patches)\n",
        "    tokens = jnp.stack([jnp.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
        "    tokens += self.pos_encodings().repeat(self.batch_size, 0)\n",
        "    \n",
        "    # Tranformer Encoder\n",
        "    for i in range(self.transformer_layers):\n",
        "      tokens = self.norm1(tokens)\n",
        "\n",
        "\n",
        "      out = tokens + self.self_attention(tokens)\n",
        "\n",
        "      out_temp = self.norm2(out)\n",
        "      out_temp = self.enc_mlp(out_temp)\n",
        "      out += out_temp\n",
        "\n",
        "    out =  out[:, 0]\n",
        "\n",
        "    logits = self.logits_mlp(out)\n",
        "    logits = nn.softmax(logits)\n",
        "    return logits\n",
        "    "
      ],
      "metadata": {
        "id": "GnL6EwwRfIOw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(patch_size=PATCH_SIZE, stride=STRIDE, image_size=IMAGE_SIZE, projection_dims=PROJECTION_DIMS, \n",
        "                          atten_heads=SELFA_HEADS, transformer_layers=TRANSFORMER_LAYERS, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "ibj3rndWgP1m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingLoop:\n",
        "  \"\"\"OOP wrapper around functional training loop.\n",
        "  \n",
        "  Args:\n",
        "    model: VisionTransformer. Model architecture for Vision Transformer.\n",
        "    train_gen: DataLoader. Dataloader to feed the training data to the model dynamically.\n",
        "    seed: int. Seed value for random number generator to ensure reproducibility.\n",
        "    epochs: int. Maximum number of iteration for training.\n",
        "    learning_rate: float. Learning rate for the optimizer.\n",
        "    momentum: float. Momentum for the optimizer.\n",
        "    val_gen: DataLoader. Default=None. Dataloader to feed the validation data to the model dynamically.\n",
        "  \n",
        "  TODO: Make the `apply_model` method jittable.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, model: VisionTransformer, train_gen: DataLoader, seed: int, epochs: int, learning_rate: float, momentum: float, val_gen=None):\n",
        "    self.model = model\n",
        "    self.train_gen = train_gen\n",
        "    self.key = seed\n",
        "    self.rng = jax.random.PRNGKey(self.key)\n",
        "    self.main_rng, self.init_rng = random.split(self.rng, 2)\n",
        "    self.epochs = epochs\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "\n",
        "    self.full_batch_size = (self.model.batch_size, self.model.image_size[0],\n",
        "                            self.model.image_size[1], self.model.image_size[2])\n",
        "    \n",
        "    self.init_train_state()\n",
        "\n",
        "\n",
        "  def init_train_state(self):\n",
        "    \"\"\"Initializes the model's and optimizer's state\n",
        "    \"\"\"\n",
        "    self.variables = self.model.init({'params': self.init_rng}, jnp.ones(self.full_batch_size))['params']\n",
        "    self.optimizer = optax.adam(self.lr, self.momentum)\n",
        "    self.train_state = train_state.TrainState.create(apply_fn = self.model.apply, tx=self.optimizer, params=self.variables)\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  def apply_model(state: train_state.TrainState, model: VisionTransformer, images: jnp.ndarray, labels: jnp.ndarray):  \n",
        "    \"\"\"Calculates the gradients during backpropogation to adjust model's parameters.\n",
        "    \n",
        "    Args:\n",
        "      state: train_state.TrainState. State of the model's params at a particular time.\n",
        "      model: VisionTransformer. Model architecture for Vision Transformer.\n",
        "      images: jnp.ndarray. Input images.\n",
        "      labels: jnp.ndarray. Labels for input images.\n",
        "    \n",
        "    Returns:\n",
        "      grads: flax.core.frozen_dict.FrozenDict. Gradients from backpropogation to update model's params.\n",
        "      loss: float. Loss function's output value.\n",
        "      accuracy: float. Accuracy of the model.\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "      \"\"\"categorical-cross entropy loss function\n",
        "      \n",
        "      Args:\n",
        "        params: . Model's params (weights and biases).\n",
        "      \n",
        "      Returns:\n",
        "        loss: float. Loss function's output value.\n",
        "        logits: jnp.ndarray. Predictions made by the `model`.\n",
        "      \"\"\"\n",
        "      logits = model.apply({'params': params}, images)\n",
        "      one_hot = jax.nn.one_hot(labels, 10)\n",
        "      loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "      return loss, logits\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "    return grads, loss, accuracy\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  @jax.jit\n",
        "  def update_model(state: train_state.TrainState, grads: flax.core.frozen_dict.FrozenDict):\n",
        "    \"\"\"Updates model's params using calculated gradients.\n",
        "    \n",
        "    Args:\n",
        "      state: train_state.TrainState. State of the model's params at a particular time.\n",
        "      grads: flax.core.frozen_dict.FrozenDict. Gradients from backpropogation to update model's params.\n",
        "    \n",
        "    Returns:\n",
        "      state: train_state.TrainState. Updated state.\n",
        "    \"\"\"\n",
        "    return state.apply_gradients(grads=grads)\n",
        "\n",
        " \n",
        "  @staticmethod\n",
        "  def train_epoch(state: train_state.TrainState, model: VisionTransformer, gen: DataLoader, batch_size: int, rng: jnp.ndarray):\n",
        "    \"\"\"Trains the model for one epoch with batch mode.\n",
        "    \n",
        "    Args:\n",
        "      state: train_state.TrainState. State of the model's params at a particular time.\n",
        "      model: VisionTransformer. Model architecture for Vision Transformer.\n",
        "      gen: DataLoader. Dataloader to feed the training data to the model dynamically.\n",
        "      batch_size: int. Size of a batch to yield.\n",
        "      rng: jnp.ndarray. Random number seed to ensure reproducibility.\n",
        "    \n",
        "    Returns:\n",
        "      state: train_state.TrainState. State of the model's params at a particular time.\n",
        "      train_loss. float. Loss for 1 epoch.\n",
        "      train_accuracy. float. Accuracy achieved for 1 epoch.\n",
        "    \"\"\"\n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "    for (batch_images, batch_labels) in tqdm(gen, desc='Batch Training', leave=False):\n",
        "      batch_images = batch_images\n",
        "      batch_labels = batch_labels\n",
        "      grads, loss, accuracy = TrainingLoop.apply_model(state, model, batch_images, batch_labels)\n",
        "      state = TrainingLoop.update_model(state, grads)\n",
        "      epoch_loss.append(loss)\n",
        "      epoch_accuracy.append(accuracy)\n",
        "\n",
        "    \n",
        "    train_loss = np.mean(epoch_loss)\n",
        "    train_accuracy = np.mean(epoch_accuracy)\n",
        "    return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def train(obj):\n",
        "    \"\"\"Drives the training process using created OOP wrapper.\n",
        "    \n",
        "    Args:\n",
        "      obj: TrainingLoop. Instance of the class `TrainingLoop` which drives the training process of the model.\n",
        "      \n",
        "    Returns:\n",
        "      obj.train_state. Final state of model's params for getting inference.\n",
        "    \"\"\"\n",
        "    for epoch in tqdm(range(1, obj.epochs + 1), desc=\"Training\"):\n",
        "      obj.train_state, train_loss, train_accuracy = TrainingLoop.train_epoch(obj.train_state,\n",
        "                                                      obj.model,\n",
        "                                                      obj.train_gen,\n",
        "                                                      obj.model.batch_size,\n",
        "                                                      obj.main_rng,\n",
        "                                                      )\n",
        "      \n",
        "      print(f\"epoch: {epoch}, train_loss: {train_loss}, train_accuracy: {train_accuracy}\")\n",
        "\n",
        "    return obj.train_state\n"
      ],
      "metadata": {
        "id": "RhPReYpUgTMQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainingLoop(model, gen, SEED, EPOCHS, LEARNING_RATE, MOMENTUM, val_gen=None)\n",
        "final_state = TrainingLoop.train(trainer)"
      ],
      "metadata": {
        "id": "922pUO0kCtNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3kR-EK1dChng"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}