{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision Transformers Flax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOXFL0QLaHr6bs/22+GVtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayureshagashe2105/GSoC-22-TensorFlow-Resources-and-Notebooks/blob/main/JAX/Vision_Transformers_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwdkoZW6XkdA",
        "outputId": "d031d9be-509e-4f36-a7d9-d0a0741426bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 136 kB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pip jax jaxlib\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import lax, random, numpy as jnp, jit\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.core.lift import vmap\n",
        "# from flax.linen.transforms import jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "q-nusa4rdzsE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATCH_SIZE = (7, 7)\n",
        "STRIDE = 7\n",
        "IMAGE_SIZE = (28, 28, 1)\n",
        "PROJECTION_DIMS = 8\n",
        "SELFA_HEADS = 2\n",
        "TRANSFORMER_LAYERS = 8\n",
        "BATCH_SIZE = 64\n",
        "NUM_CLASSES = 10\n",
        "LEARNING_RATE = 0.001\n",
        "MOMENTUM = 0.9\n",
        "SEED = 0\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "NCJtk-XAgXym"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(tf.keras.utils.Sequence):\n",
        "  def __init__(self, batch_size, X, y, is_training):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.batch_size = batch_size\n",
        "    self.is_training = is_training\n",
        "    self.indices = range(X.shape[0])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.shape[0] // self.batch_size\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    batch_indices = self.indices[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "    batch_images = self.X[batch_indices]\n",
        "    batch_labels = self.y[batch_indices]\n",
        "\n",
        "    if(len(batch_images.shape) == 3):\n",
        "      batch_images = jnp.expand_dims(batch_images, -1)\n",
        "\n",
        "    return jnp.array(batch_images), jnp.array(batch_labels)\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = ((tf.cast(X_train, tf.float32).numpy() / 255.0, tf.cast(y_train, tf.int32).numpy()), \n",
        "                                        (tf.cast(X_test, tf.float32).numpy() / 255.0, tf.cast(y_test, tf.int32).numpy()))\n",
        "\n",
        "\n",
        "gen = DataLoader(BATCH_SIZE, X_train[:2000], y_train[:2000], True)"
      ],
      "metadata": {
        "id": "U71njssJeX-A"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \n",
        "  hidden_layer_nodes: Sequence[int]\n",
        "\n",
        "  def setup(self):\n",
        "\n",
        "    self.layers = [nn.Dense(n) for n in self.hidden_layer_nodes]\n",
        "  \n",
        "  @nn.compact\n",
        "  def __call__(self, input):\n",
        "    for layer in self.layers:\n",
        "      x = layer(input)\n",
        "      x = self.apply_activation(x)\n",
        "      return x\n",
        "  \n",
        "\n",
        "  def apply_activation(self, input):\n",
        "    return nn.gelu(input)"
      ],
      "metadata": {
        "id": "_KGSRTARe8Rm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchExtractor(nn.Module):\n",
        "  patch_size: Sequence[int]\n",
        "  stride: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, images):\n",
        "    # print(images.shape)\n",
        "    patches = jax.lax.conv_general_dilated_patches(images[:, None, None, :], \n",
        "                                                   (1, self.patch_size[0], self.patch_size[1], 1), \n",
        "                                                   (1, self.stride, self.stride, 1), \n",
        "                                                   padding=\"VALID\")\n",
        "    n_patches = (images.shape[1] // self.patch_size[0]) * (images.shape[2] // self.patch_size[1])\n",
        "    patch_dims = self.patch_size[0] * self.patch_size[1] * images.shape[3]\n",
        "    image_patches = patches.reshape(images.shape[0], n_patches, patch_dims)\n",
        "\n",
        "    return image_patches"
      ],
      "metadata": {
        "id": "EFKzPHOSWcSz"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(nn.Module):\n",
        "  num_patches: int\n",
        "  projection_dims: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.projection = nn.Dense(self.projection_dims)\n",
        "    self.positional_encodings = nn.Embed(self.num_patches, self.projection_dims)\n",
        "  \n",
        "  @nn.compact\n",
        "  def __call__(self, patch):\n",
        "    positions = jnp.arange(0, self.num_patches)\n",
        "    encode = self.projection(patch) + self.positional_encodings(positions)\n",
        "    return encode"
      ],
      "metadata": {
        "id": "k1fzvHoLfFEm"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncodings(nn.Module):\n",
        "  seq_len: int\n",
        "  num_heads: int\n",
        "\n",
        "  def __call__(self):\n",
        "    res = jnp.ones((self.seq_len, self.num_heads))\n",
        "    for i in range(self.seq_len):\n",
        "      for j in range(self.num_heads):\n",
        "        res = res.at[i, j].set(jnp.sin(i / (10000 ** (j / self.num_heads))) if j % 2 == 0 else jnp.cos(i / (10000 ** ((j - 1) / self.num_heads))))\n",
        "    \n",
        "    return jnp.expand_dims(res, 0)"
      ],
      "metadata": {
        "id": "hWGtvTSHjFKV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  patch_size: Sequence[int]\n",
        "  stride: int\n",
        "  image_size: Sequence[int]\n",
        "  projection_dims: int\n",
        "  atten_heads: int\n",
        "  transformer_layers: int\n",
        "  batch_size: int\n",
        "  num_classes: int\n",
        "\n",
        "\n",
        "  def setup(self):\n",
        "    self.patchify = PatchExtractor(self.patch_size, self.stride)\n",
        "    self.patch_dims = self.patch_size[0] * self.patch_size[1] * self.image_size[-1]\n",
        "    self.tokens = MLP([self.patch_dims, self.projection_dims][::-1])\n",
        "    self.class_token = self.param(\"class_token\", lambda rng, shape: random.normal(rng, shape), (1, self.projection_dims))\n",
        "\n",
        "    self.num_patches = ((self.image_size[0] - self.patch_size[0]) // self.stride + 1) * ((self.image_size[1] - self.patch_size[1]) // self.stride + 1)\n",
        "    self.pos_encodings = PositionalEncodings(self.num_patches + 1, self.projection_dims)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(epsilon=1e-6)\n",
        "    self.self_attention = nn.SelfAttention(self.atten_heads, qkv_features=self.projection_dims)\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(epsilon=1e-6)\n",
        "    self.enc_mlp = MLP([self.projection_dims, self.projection_dims])\n",
        "\n",
        "    self.logits_mlp = MLP([self.num_classes, self.projection_dims])\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    image_patches = self.patchify(inputs)\n",
        "    tokens = self.tokens(image_patches)\n",
        "    tokens = jnp.stack([jnp.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
        "    tokens += self.pos_encodings().repeat(self.batch_size, 0)\n",
        "    tokens = self.norm1(tokens)\n",
        "\n",
        "\n",
        "    out = tokens + self.self_attention(tokens)\n",
        "\n",
        "    out_temp = self.norm2(out)\n",
        "    out_temp = self.enc_mlp(out_temp)\n",
        "    out_temp = nn.relu(out_temp)\n",
        "    out += out_temp\n",
        "\n",
        "    out =  out[:, 0]\n",
        "\n",
        "    logits = self.logits_mlp(out)\n",
        "    logits = nn.softmax(logits)\n",
        "    return logits\n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        "  @staticmethod\n",
        "  @jit\n",
        "  def layer_add(x, y):\n",
        "    return jnp.add(x, y)"
      ],
      "metadata": {
        "id": "GnL6EwwRfIOw"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(patch_size=PATCH_SIZE, stride=STRIDE, image_size=IMAGE_SIZE, projection_dims=PROJECTION_DIMS, \n",
        "                          atten_heads=SELFA_HEADS, transformer_layers=TRANSFORMER_LAYERS, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "id": "ibj3rndWgP1m"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingLoop:\n",
        "  \n",
        "  def __init__(self, model, train_gen, seed, epochs, learning_rate, momentum, test_gen=None):\n",
        "    self.model = model\n",
        "    self.train_gen = train_gen\n",
        "    self.key = seed\n",
        "    self.rng = jax.random.PRNGKey(self.key)\n",
        "    self.main_rng, self.init_rng = random.split(self.rng, 2)\n",
        "    self.epochs = epochs\n",
        "    self.lr = learning_rate\n",
        "    self.momentum = momentum\n",
        "\n",
        "    self.full_batch_size = (self.model.batch_size, self.model.image_size[0],\n",
        "                            self.model.image_size[1], self.model.image_size[2])\n",
        "    \n",
        "    self.init_train_state()\n",
        "\n",
        "\n",
        "\n",
        "  def init_train_state(self):\n",
        "    self.variables = self.model.init({'params': self.init_rng}, jnp.ones(self.full_batch_size))['params']\n",
        "    self.optimizer = optax.adam(self.lr, self.momentum)\n",
        "    self.train_state = train_state.TrainState.create(apply_fn = self.model.apply, tx=self.optimizer, params=self.variables)\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  def apply_model(state, model, images, labels, num_classes):  \n",
        "    \n",
        "    def loss_fn(params):\n",
        "      logits = model.apply({'params': params}, images)\n",
        "      one_hot = jax.nn.one_hot(labels, 10)\n",
        "      loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
        "      return loss, logits\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
        "    return grads, loss, accuracy\n",
        "\n",
        "  \n",
        "  @staticmethod\n",
        "  @jax.jit\n",
        "  def update_model(state, grads):\n",
        "    return state.apply_gradients(grads=grads)\n",
        "\n",
        " \n",
        "  @staticmethod\n",
        "  def train_epoch(state, model, gen, batch_size, rng, num_classes):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "\n",
        "    for (batch_images, batch_labels) in tqdm(gen, desc='Training', leave=False):\n",
        "      batch_images = batch_images\n",
        "      batch_labels = batch_labels\n",
        "      grads, loss, accuracy = TrainingLoop.apply_model(state, model, batch_images, batch_labels, 10)\n",
        "      state = TrainingLoop.update_model(state, grads)\n",
        "      epoch_loss.append(loss)\n",
        "      epoch_accuracy.append(accuracy)\n",
        "    \n",
        "    train_loss = np.mean(epoch_loss)\n",
        "    train_accuracy = np.mean(epoch_accuracy)\n",
        "    return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def train(obj):\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(1, obj.epochs + 1), desc=\"Epoch\"):\n",
        "      obj.train_state, train_loss, train_accuracy = TrainingLoop.train_epoch(obj.train_state,\n",
        "                                                      obj.model,\n",
        "                                                      obj.train_gen,\n",
        "                                                      obj.model.batch_size,\n",
        "                                                      obj.main_rng,\n",
        "                                                      obj.model.num_classes)\n",
        "      \n",
        "      print(\n",
        "          'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f'\n",
        "          % (epoch, train_loss, train_accuracy * 100)\n",
        "          )\n",
        "\n",
        "    return obj.train_state\n"
      ],
      "metadata": {
        "id": "RhPReYpUgTMQ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainingLoop(model, gen, SEED, EPOCHS, LEARNING_RATE, MOMENTUM, test_gen=None)\n",
        "final_state = TrainingLoop.train(trainer)"
      ],
      "metadata": {
        "id": "922pUO0kCtNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c8gIwK9dcKfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}